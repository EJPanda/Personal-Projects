{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Neural Machine Translation of Languages\n",
    "\n",
    "Project Status: On-going. Working on NMT using RNN to translate English into French.\n",
    "\n",
    "This project is a proof of concept project to study neural machine translation (NMT) by making deep learning models translating the English language into the French language. In the future, I plan to see the how the model gets affected by reversing the source and target languages. I also plan to expand this project to include additional languages.\n",
    "\n",
    "#### By: Eric J. Puttock\n",
    "#### Last Updated: 2020.09.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in all libraries that will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libaries needed for this project here all at once. Gives you an overview of all packages used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import RepeatVector, GRU, TimeDistributed\n",
    "from keras.layers.core import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_en.txt','r', encoding=\"utf-8\") as f_en:\n",
    "    en_text = f_en.readlines()\n",
    "with open('vocab_fr.txt','r', encoding=\"utf-8\") as f_fr:\n",
    "    fr_text = f_fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_en_text = en_text.copy()\n",
    "backup_fr_text = fr_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "filters = punctuation +'\\n'+'>'+'<'+'+'\n",
    "def RemovePunctutation(sentence):\n",
    "    return ''.join(s for s in sentence if s not in filters)\n",
    "def RemoveSpace(sentence):\n",
    "    return re.sub('  ',' ',sentence).strip().lower()\n",
    "def FilterProcess(sentence):\n",
    "    return RemoveSpace(RemovePunctutation(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = list(map(FilterProcess,en_text))\n",
    "fr_text = list(map(FilterProcess,fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = ['sos '+sent+' eos' for sent in en_text]\n",
    "fr_text = ['sos '+sent+' eos' for sent in fr_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137860 137860\n"
     ]
    }
   ],
   "source": [
    "print(len(en_text),len(fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Mean sentence length:  13.26144639489337\n"
     ]
    }
   ],
   "source": [
    "en_sent_lengths = [len(en_sent.split(\" \")) for en_sent in en_text]\n",
    "en_mean_length = np.mean(en_sent_lengths)\n",
    "print('(English) Mean sentence length: ', en_mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Vocabulary size:  201\n"
     ]
    }
   ],
   "source": [
    "en_all_words = []\n",
    "for sent in en_text:\n",
    "    en_all_words.extend(sent.split(\" \"))\n",
    "en_vocab_size = len(set(en_all_words))\n",
    "print(\"(English) Vocabulary size: \", en_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(French) Mean sentence length:  14.34153489046859\n"
     ]
    }
   ],
   "source": [
    "fr_sent_lengths = [len(fr_sent.split(\" \")) for fr_sent in fr_text]\n",
    "fr_mean_length = np.mean(fr_sent_lengths)\n",
    "print('(French) Mean sentence length: ', fr_mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(French) Vocabulary size:  347\n"
     ]
    }
   ],
   "source": [
    "fr_all_words = []\n",
    "for sent in fr_text:\n",
    "    fr_all_words.extend(sent.split(\" \"))\n",
    "fr_vocab_size = len(set(fr_all_words))\n",
    "print(\"(French) Vocabulary size: \", fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_=plt.hist(en_sent_lengths, alpha = 0.5, density = True)\n",
    "_=plt.hist(fr_sent_lengths, alpha = 0.5, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data into Train, Validation, Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68241 35154 34465\n"
     ]
    }
   ],
   "source": [
    "# Split the data between training, validation, and hold-off test sets.\n",
    "\n",
    "# Initialize index set, and randomize it. Then, sample the sets.\n",
    "inds = np.arange(len(en_text))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "TestValidSize = round(len(en_text)*.75)\n",
    "test_size = len(en_text)-TestValidSize\n",
    "\n",
    "train_size = round(TestValidSize*.66)\n",
    "valid_size = TestValidSize - train_size\n",
    "#train_size, valid_size = 800, 200\n",
    "\n",
    "print(train_size, valid_size, test_size)\n",
    "\n",
    "# Training Set:\n",
    "train_inds = inds[:train_size]\n",
    "valid_inds = inds[train_size:train_size+valid_size]\n",
    "tr_en = [en_text[ti] for ti in train_inds]\n",
    "tr_fr = [fr_text[ti] for ti in train_inds]\n",
    "\n",
    "# Validation Set:\n",
    "v_en = [en_text[ti] for ti in valid_inds]\n",
    "v_fr = [fr_text[ti] for ti in valid_inds]\n",
    "\n",
    "# Test Set:\n",
    "test_inds = inds[train_size+valid_size:]\n",
    "te_en = [en_text[ti] for ti in test_inds]\n",
    "te_fr = [fr_text[ti] for ti in test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Train Mean sentence length:  13.258012045544467\n"
     ]
    }
   ],
   "source": [
    "Tr_en_sent_lengths = [len(en_sent.split(\" \")) for en_sent in tr_en]\n",
    "Tr_en_mean_length = np.mean(Tr_en_sent_lengths)\n",
    "print('(English) Train Mean sentence length: ', Tr_en_mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Train Vocabulary size:  201\n"
     ]
    }
   ],
   "source": [
    "Tr_en_all_words = []\n",
    "for sent in tr_en:\n",
    "    Tr_en_all_words.extend(sent.split(\" \"))\n",
    "Tr_en_vocab_size = len(set(Tr_en_all_words))\n",
    "print(\"(English) Train Vocabulary size: \", Tr_en_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(French) Train Mean sentence length:  14.34430913966677\n"
     ]
    }
   ],
   "source": [
    "Tr_fr_sent_lengths = [len(fr_sent.split(\" \")) for fr_sent in tr_fr]\n",
    "Tr_fr_mean_length = np.mean(Tr_fr_sent_lengths)\n",
    "print('(French) Train Mean sentence length: ', Tr_fr_mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(French) Train Vocabulary size:  342\n"
     ]
    }
   ],
   "source": [
    "Tr_fr_all_words = []\n",
    "for sent in tr_fr:\n",
    "    Tr_fr_all_words.extend(sent.split(\" \"))\n",
    "Tr_fr_vocab_size = len(set(Tr_fr_all_words))\n",
    "print(\"(French) Train Vocabulary size: \", Tr_fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_=plt.hist(Tr_en_sent_lengths, alpha = 0.5, density = True)\n",
    "_=plt.hist(Tr_fr_sent_lengths, alpha = 0.5, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Hyperparamters:\n",
    "en_len = 20\n",
    "en_vocab = 200\n",
    "\n",
    "fr_len = 20\n",
    "fr_vocab = 350\n",
    "\n",
    "en_hsize = 64\n",
    "fr_hsize = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_tok = Tokenizer()\n",
    "en_tok = Tokenizer(num_words = en_vocab+1, oov_token = 'UNK')\n",
    "en_tok.fit_on_texts(tr_en)\n",
    "\n",
    "#fr_tok = Tokenizer()\n",
    "fr_tok = Tokenizer(num_words = fr_vocab+1, oov_token = 'UNK')\n",
    "fr_tok.fit_on_texts(tr_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "print(len(en_tok.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "print(len(fr_tok.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark: \n",
    "This is a bug in Tokenizer that is addressed on GitHub: https://github.com/keras-team/keras/issues/8092 .\n",
    "- The number of words in the tokenizer is not equal to the num_words we originally wanted.\n",
    "- We need to manually sort this out (at the expense of losing a little bit) by following the procedure described in the link.\n",
    "- We will also have to update index_word accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok.word_index = {e:i for e,i in en_tok.word_index.items() if i <= en_vocab} # <= because tokenizer is 1 indexed\n",
    "en_tok.word_index[en_tok.oov_token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok.index_word = {v:k for k,v in en_tok.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tok.word_index = {e:i for e,i in fr_tok.word_index.items() if i <= fr_vocab} # <= because tokenizer is 1 indexed\n",
    "fr_tok.word_index[fr_tok.oov_token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tok.index_word = {v:k for k,v in fr_tok.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(en_tok.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "print(len(fr_tok.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sos new jersey is sometimes quiet during autumn and it is snowy in april eos\n",
      "sos new jersey is sometimes quiet during autumn and it is snowy in april eos\n",
      "\n",
      "\tsos new jersey est parfois calme pendant l automne et il est neigeux en avril eos\n",
      "\tsos new jersey est parfois calme pendant l automne et il est neigeux en avril eos\n",
      "\n",
      "sos the united states is usually chilly during july and it is usually freezing in november eos\n",
      "sos the united states is usually chilly during july and it is usually freezing in november eos\n",
      "\n",
      "\tsos les étatsunis est généralement froid en juillet et il gèle habituellement en novembre eos\n",
      "\tsos les étatsunis est généralement froid en juillet et il gèle habituellement en novembre eos\n",
      "\n",
      "sos california is usually quiet during march and it is usually hot in june eos\n",
      "sos california is usually quiet during march and it is usually hot in june eos\n",
      "\n",
      "\tsos california est généralement calme en mars et il est généralement chaud en juin eos\n",
      "\tsos california est généralement calme en mars et il est généralement chaud en juin eos\n",
      "\n",
      "sos the united states is sometimes mild during june and it is cold in september eos\n",
      "sos the united states is sometimes mild during june and it is cold in september eos\n",
      "\n",
      "\tsos les étatsunis est parfois légère en juin et il fait froid en septembre eos\n",
      "\tsos les étatsunis est parfois légère en juin et il fait froid en septembre eos\n",
      "\n",
      "sos your least liked fruit is the grape but my least liked is the apple eos\n",
      "sos your least liked fruit is the grape but my least liked is the apple eos\n",
      "\n",
      "\tsos votre moins aimé fruit est le raisin mais mon moins aimé est la pomme eos\n",
      "\tsos votre moins aimé fruit est le raisin mais mon moins aimé est la pomme eos\n",
      "\n",
      "sos his favorite fruit is the orange but my favorite is the grape eos\n",
      "sos his favorite fruit is the orange but my favorite is the grape eos\n",
      "\n",
      "\tsos son fruit préféré est lorange mais mon préféré est le raisin eos\n",
      "\tsos son fruit préféré est lorange mais mon préféré est le raisin eos\n",
      "\n",
      "sos paris is relaxing during december but it is usually chilly in july eos\n",
      "sos paris is relaxing during december but it is usually chilly in july eos\n",
      "\n",
      "\tsos paris est relaxant en décembre mais il est généralement froid en juillet eos\n",
      "\tsos paris est relaxant en décembre mais il est généralement froid en juillet eos\n",
      "\n",
      "sos new jersey is busy during spring and it is never hot in march eos\n",
      "sos new jersey is busy during spring and it is never hot in march eos\n",
      "\n",
      "\tsos new jersey est occupé au printemps et il est jamais chaude en mars eos\n",
      "\tsos new jersey est occupé au printemps et il est jamais chaude en mars eos\n",
      "\n",
      "sos our least liked fruit is the lemon but my least liked is the grape eos\n",
      "sos our least liked fruit is the lemon but my least liked is the grape eos\n",
      "\n",
      "\tsos notre fruit est moins aimé le citron mais mon moins aimé est le raisin eos\n",
      "\tsos notre fruit est moins aimé le citron mais mon moins aimé est le raisin eos\n",
      "\n",
      "sos the united states is sometimes busy during january and it is sometimes warm in november eos\n",
      "sos the united states is sometimes busy during january and it is sometimes warm in november eos\n",
      "\n",
      "\tsos les étatsunis est parfois occupé en janvier et il est parfois chaud en novembre eos\n",
      "\tsos les étatsunis est parfois occupé en janvier et il est parfois chaud en novembre eos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,10):\n",
    "    print(en_text[j])\n",
    "    print(' '.join(en_tok.index_word[s] for s in en_tok.texts_to_sequences([en_text[j]])[0]) + '\\n')\n",
    "    print('\\t' + fr_text[j])\n",
    "    print('\\t' + ' '.join(fr_tok.index_word[s] for s in fr_tok.texts_to_sequences([fr_text[j]])[0])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, we appropriately fixed it. Words not in the top fr_vocab in frequency, they should become UNK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new keyword parameter reverse which defaults to False\n",
    "def sents2seqs(sentences, lang='en', pad_type='post', reverse=False, onehot=False):     \n",
    "    if lang == 'en':\n",
    "        encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "        preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "        if reverse:\n",
    "            # Reverse the text using numpy axis reversing\n",
    "            preproc_text = preproc_text[:,::-1]\n",
    "        if onehot: # +1 (added padding for 0's) + 1 (added 'UNK')\n",
    "            preproc_text = to_categorical(preproc_text, num_classes=en_vocab+1+1)\n",
    "    \n",
    "    if lang == 'fr':\n",
    "        encoded_text = fr_tok.texts_to_sequences(sentences)\n",
    "        preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=fr_len)\n",
    "        if reverse:\n",
    "            # Reverse the text using numpy axis reversing\n",
    "            preproc_text = preproc_text[:,::-1]\n",
    "        if onehot: # +1 (added padding for 0's) + 1 (added 'UNK')\n",
    "            preproc_text = to_categorical(preproc_text, num_classes=fr_vocab+1+1)\n",
    "    \n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Train data to onehot\n",
    "#tr_en_x = sents2seqs('source', v_en, onehot=True, pad_type='pre')\n",
    "tr_en_x = sents2seqs(tr_en, lang = 'en', onehot=True, reverse=True)\n",
    "tr_de_y = sents2seqs(tr_fr, lang = 'fr', onehot=True)\n",
    "\n",
    "# Convert validation data to onehot\n",
    "#v_en_x = sents2seqs('source', v_en, onehot=True, pad_type='pre')\n",
    "v_en_x = sents2seqs(v_en, lang = 'en', onehot=True, reverse=True)\n",
    "v_de_y = sents2seqs(v_fr, lang = 'fr', onehot=True)\n",
    "\n",
    "# Convert Test data to onehot\n",
    "#te_en_x = sents2seqs('source', v_en, onehot=True, pad_type='pre')\n",
    "te_en_x = sents2seqs(te_en, lang = 'en', onehot=True, reverse=True)\n",
    "te_de_y = sents2seqs(te_fr, lang = 'fr', onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68241, 20, 202) (68241, 20, 352)\n",
      "(35154, 20, 202) (35154, 20, 352)\n",
      "(34465, 20, 202) (34465, 20, 352)\n",
      "137860 137860\n"
     ]
    }
   ],
   "source": [
    "print(tr_en_x.shape, tr_de_y.shape)\n",
    "print(v_en_x.shape, v_de_y.shape)\n",
    "print(te_en_x.shape, te_de_y.shape)\n",
    "print(len(en_text), len(fr_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 202)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 64), (None,  51456       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 20, 64)       0           gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 20, 64)       24960       repeat_vector[0][0]              \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 20, 352)      22880       gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 99,296\n",
      "Trainable params: 99,296\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ENCODER GRU:\n",
    "en_inputs = keras.layers.Input(shape=(en_len, en_vocab+1+1))\n",
    "en_out, en_state = keras.layers.GRU(en_hsize, return_state=True)(en_inputs)\n",
    "\n",
    "# DECODER GRU:\n",
    "de_inputs = keras.layers.RepeatVector(fr_len)(en_state)\n",
    "de_gru = keras.layers.GRU(fr_hsize, return_sequences=True)\n",
    "de_out = de_gru(de_inputs, initial_state=en_state)\n",
    "\n",
    "# Decoder Prediction layer:\n",
    "de_dense = keras.layers.Dense(fr_vocab+1+1, activation='softmax')\n",
    "\n",
    "de_dense_time = keras.layers.TimeDistributed(de_dense)\n",
    "de_pred = de_dense_time(de_out)\n",
    "# de_pred holds the probability predictions over all French words --- for each position of the decoder.\n",
    "\n",
    "nmt = keras.models.Model(inputs=en_inputs, outputs=de_pred)\n",
    "\n",
    "print(nmt.summary())\n",
    "\n",
    "nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step - loss: 2.2152 - acc: 0.5018\n",
      "Epoch: 1 => Loss:2.2151575088500977, Val Acc: 50.18276572227478\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.8707 - acc: 0.5417\n",
      "Epoch: 2 => Loss:1.8706599473953247, Val Acc: 54.171931743621826\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.6975 - acc: 0.5597\n",
      "Epoch: 3 => Loss:1.697473406791687, Val Acc: 55.96902370452881\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5742 - acc: 0.5840\n",
      "Epoch: 4 => Loss:1.5741972923278809, Val Acc: 58.396339416503906\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4729 - acc: 0.6055\n",
      "Epoch: 5 => Loss:1.4728634357452393, Val Acc: 60.54816246032715\n"
     ]
    }
   ],
   "source": [
    "n_epochs, bsize = 5, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        # Get a single batch of inputs and outputs\n",
    "        en_x = tr_en_x[i:i+bsize]\n",
    "        #en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, pad_type='pre')\n",
    "        de_y = tr_de_y[i:i+bsize]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt.train_on_batch(en_x, de_y)    \n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"Epoch: {} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 => Loss:2.6829893589019775, Val Acc: 40.41062295436859\n",
      "Epoch: 2 => Loss:2.2278196811676025, Val Acc: 49.8392790555954\n",
      "Epoch: 3 => Loss:1.8398823738098145, Val Acc: 53.5509467124939\n",
      "Epoch: 4 => Loss:1.6721118688583374, Val Acc: 55.8070182800293\n",
      "Epoch: 5 => Loss:1.5646345615386963, Val Acc: 57.94305205345154\n",
      "Epoch: 6 => Loss:1.4587271213531494, Val Acc: 60.1526141166687\n",
      "Epoch: 7 => Loss:1.3834117650985718, Val Acc: 62.187659740448\n",
      "Epoch: 8 => Loss:1.3207706212997437, Val Acc: 63.696449995040894\n",
      "Epoch: 9 => Loss:1.2597321271896362, Val Acc: 64.78622555732727\n",
      "Epoch: 10 => Loss:1.2045804262161255, Val Acc: 66.33298993110657\n"
     ]
    }
   ],
   "source": [
    "n_epochs, bsize = 10, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        # Get a single batch of inputs and outputs\n",
    "        en_x = tr_en_x[i:i+bsize]\n",
    "        #en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, pad_type='pre')\n",
    "        de_y = tr_de_y[i:i+bsize]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt.train_on_batch(en_x, de_y)    \n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"Epoch: {} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 => Loss:1.1518031358718872, Val Acc: 67.84007549285889\n",
      "Epoch: 2 => Loss:1.1059176921844482, Val Acc: 68.90382170677185\n",
      "Epoch: 3 => Loss:1.0623801946640015, Val Acc: 70.01422047615051\n",
      "Epoch: 4 => Loss:1.0172191858291626, Val Acc: 71.2810754776001\n",
      "Epoch: 5 => Loss:0.977175772190094, Val Acc: 72.28935360908508\n",
      "Epoch: 6 => Loss:0.9520580172538757, Val Acc: 72.98429608345032\n",
      "Epoch: 7 => Loss:0.917051374912262, Val Acc: 73.79088997840881\n",
      "Epoch: 8 => Loss:0.9005306363105774, Val Acc: 74.17349219322205\n",
      "Epoch: 9 => Loss:0.8652517199516296, Val Acc: 75.09245276451111\n",
      "Epoch: 10 => Loss:0.8474580645561218, Val Acc: 75.57347416877747\n"
     ]
    }
   ],
   "source": [
    "n_epochs, bsize = 10, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        # Get a single batch of inputs and outputs\n",
    "        en_x = tr_en_x[i:i+bsize]\n",
    "        #en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, pad_type='pre')\n",
    "        de_y = tr_de_y[i:i+bsize]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt.train_on_batch(en_x, de_y)    \n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"Epoch: {} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 => Loss:0.8309590220451355, Val Acc: 76.01510286331177\n",
      "Epoch: 2 => Loss:0.8075124621391296, Val Acc: 76.67036652565002\n",
      "Epoch: 3 => Loss:0.8014142513275146, Val Acc: 76.8064796924591\n",
      "Epoch: 4 => Loss:0.782507598400116, Val Acc: 77.354496717453\n",
      "Epoch: 5 => Loss:0.7522270083427429, Val Acc: 78.24557423591614\n",
      "Epoch: 6 => Loss:0.7394343614578247, Val Acc: 78.73528003692627\n",
      "Epoch: 7 => Loss:0.7151362895965576, Val Acc: 79.23579216003418\n",
      "Epoch: 8 => Loss:0.7023789882659912, Val Acc: 79.52921390533447\n",
      "Epoch: 9 => Loss:0.6771063804626465, Val Acc: 80.260568857193\n",
      "Epoch: 10 => Loss:0.6605172157287598, Val Acc: 80.6606650352478\n"
     ]
    }
   ],
   "source": [
    "n_epochs, bsize = 10, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        # Get a single batch of inputs and outputs\n",
    "        en_x = tr_en_x[i:i+bsize]\n",
    "        #en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, pad_type='pre')\n",
    "        de_y = tr_de_y[i:i+bsize]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt.train_on_batch(en_x, de_y)    \n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"Epoch: {} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 => Loss:0.6447697877883911, Val Acc: 81.20982050895691\n",
      "Epoch: 2 => Loss:0.6309666633605957, Val Acc: 81.63907527923584\n",
      "Epoch: 3 => Loss:0.6120499968528748, Val Acc: 82.13830590248108\n",
      "Epoch: 4 => Loss:0.6061715483665466, Val Acc: 82.48549103736877\n",
      "Epoch: 5 => Loss:0.5958191752433777, Val Acc: 82.81504511833191\n",
      "Epoch: 6 => Loss:0.5753939151763916, Val Acc: 83.37884545326233\n",
      "Epoch: 7 => Loss:0.561760425567627, Val Acc: 83.89912843704224\n",
      "Epoch: 8 => Loss:0.5470174551010132, Val Acc: 84.30036306381226\n",
      "Epoch: 9 => Loss:0.54120272397995, Val Acc: 84.5710277557373\n",
      "Epoch: 10 => Loss:0.528843343257904, Val Acc: 85.07083058357239\n"
     ]
    }
   ],
   "source": [
    "n_epochs, bsize = 10, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        # Get a single batch of inputs and outputs\n",
    "        en_x = tr_en_x[i:i+bsize]\n",
    "        #en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, pad_type='pre')\n",
    "        de_y = tr_de_y[i:i+bsize]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt.train_on_batch(en_x, de_y)    \n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"Epoch: {} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmt.save('NMT_EnFr_model.h5')\n",
    "#nmt.save_weights('NMT_EnFr_weights.h5')\n",
    "nmt.load_weights('NMT_EnFr_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078/1078 [==============================] - 18s 17ms/step - loss: 3.7814 - acc: 0.4833\n",
      "Loss:3.7814133167266846, Test Acc: 48.33497703075409\n"
     ]
    }
   ],
   "source": [
    "score = nmt.evaluate(te_en_x, te_de_y)\n",
    "print(\"Loss:{}, Test Acc: {}\".format(score[0], score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_pred = nmt.predict(te_en_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34465, 20, 352)\n"
     ]
    }
   ],
   "source": [
    "print(fr_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_seq = np.argmax(fr_pred, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34465, 20)\n"
     ]
    }
   ],
   "source": [
    "print(fr_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English :  sos the united states is sometimes quiet during july but it is hot in august eos\n",
      "\t French :  sos les étatsunis est parfois calme en juillet mais il est chaud en août eos\n",
      "\t \t Prediction :  sos paris france est généralement pluvieux aimé mois de juillet mais il est froid en en eos eos\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "print('English : ', te_en[j])\n",
    "print('\\t French : ', te_fr[j])\n",
    "print('\\t \\t Prediction : ', ' '.join([fr_tok.index_word[i] for i in fr_seq[j] if i != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English :  sos california is sometimes rainy during november and it is nice in autumn eos\n",
      "\t French :  sos california est parfois pluvieux en novembre et il est agréable à l automne eos\n",
      "\t \t Prediction :  sos la est est chaud pendant l mais il est est à à automne eos\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "print('English : ', te_en[j])\n",
    "print('\\t French : ', te_fr[j])\n",
    "print('\\t \\t Prediction : ', ' '.join([fr_tok.index_word[i] for i in fr_seq[j] if i != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English :  sos paris is sometimes pleasant during august but it is usually quiet in march eos\n",
      "\t French :  sos paris est parfois agréable au mois d août mais il est généralement calme en mars eos\n",
      "\t \t Prediction :  sos doux est généralement notre en merveilleux mais il lautomne jamais en juin eos\n"
     ]
    }
   ],
   "source": [
    "j = 2\n",
    "print('English : ', te_en[j])\n",
    "print('\\t French : ', te_fr[j])\n",
    "print('\\t \\t Prediction : ', ' '.join([fr_tok.index_word[i] for i in fr_seq[j] if i != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using one fixed validation set, we achaived 85% validation accuracy, but likely overfitted.\n",
    "# Only ~45% accuracy on test set.\n",
    "- We only did training using one set of a 3-fold cross-validation\n",
    "- We should be training on the full 3-fold CV. This should prevent overfitting and learn a little more.\n",
    "- Note from the example above, in some samples sentences most common words seem to be matched well already. However, although the model has correctly learned where to place them, specific terms such as a name of country or a fruit still needs more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this was pretty good for my first NMT project!\n",
    "\n",
    "Training & Testing needs improvement (and notebook clean-up).\n",
    "\n",
    "Proof of concept that I understand the fundamentals of encoding/decoding and being able to make a model to translate languages.\n",
    "\n",
    "Next to is modify the model parameters as well types of layers to see if a better model is attainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
